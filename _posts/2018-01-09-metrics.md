---
layout: post
title: Metrics
subtitle: Types | Differences | Takeaway
image: /img/metrics.jpeg
tags: [data science, machine learning]
comments: true
---

Metric is a measure of the effectiveness of a model. The effectiveness of a model depends on the problem type and is unique. So metric is different for all kinds of problems.

## Regression metrics

- **MSE**: Measures average/mean squared error of our predictions. At each point, calculate the squared difference between predictions and target and average those values over the objects.

    What if we do not use the features to predict target values? What if we predict the same value for all the objects? Now the optimal value that reduces the MSE is the *mean* value of the target column. This is the baseline model.

    To illustrate, consider the following target values:
    ```python
    In [7]: pd.DataFrame([5, 9, 8, 6, 27], columns = ['y'])
    Out[7]: 
        y
    0   5
    1   9
    2   8
    3   6
    4  27
    ```
    To build a baseline model, we should find that one constant value which we predict irrespective of the parameters or features. It is (55/5) i.e. 11.

- **RMSE**: Root Mean Square Error is very similar to MSE. Taking root to make the scale of errors as the same of the targets. It will be easy to comprehend the error as it is linear now.

    RMSE and MSE are similar in a way where every minimizer of MSE is a minimizer of RMSE and vice-versa.
    ```python
    MSE(a) > MSE(b) <=> RMSE(a) > RMSE(b)
    ```

    But they are different when we calculate the gradient of RMSE w.r.t i'th prediction which is equal to the gradient of MSE multiplied by some value. This value is called the *learning rate*. In simple words, it means traveling along MSE gradient is equivalent to traveling along RMSE gradient with different learning rate. 

> What would it mean to say, I have a value of 27 for MSE and 0.8 for RMSE? Should I improve my model? 

*So it is hard to just come to a conclusion with just absolute values. It depends upon properties of the dataset and the variation in the target vector. We should be interested in knowing how better is our model when compared to constant baseline model!*

- **R-squared**: This metric scores the model 1 if our model is perfect, 0 otherwise.\s\s
        If MSE is 0, R^2^ is 1.\s\s
        If MSE is 1, R^2^ is 0.\s\s
    All models will score between 0 and 1. If we optimize MSE, we are optimizing R^2^.

> The above 3 metrics are similar from an optimization point of view.

- **MAE**: Mean Absolute Error is calculated by averaging the distances between the absolute target values and the predicted values. So, it doesn't penalize heavily huge errors as in case of MSE. So, it is not as sensitive as MSE or influenced by any outliers.

    It is used in the financial domain where $10 error is double that of $5 error. In contrast to MSE thinks that $10 is 4 times worse than $5 error.

    The optimal value that reduces the MSE is the *median* value of the target column. This is the baseline model for MAE.

    To illustrate, consider the following target values:
    ```python
    In [7]: pd.DataFrame([5, 9, 8, 6, 27], columns = ['y'])
    Out[7]: 
        y
    0   5
    1   9
    2   8
    3   6
    4  27
    ```
    To build a baseline model with MAE, we should find that one constant value which we predict irrespective of the parameters or features. It is 8.

> If you are sure that data has outliers, choose MAE.\s\s
If you think they are unexpected values, and needs to be considered to build the model, choose MSE.

## Classification metrics