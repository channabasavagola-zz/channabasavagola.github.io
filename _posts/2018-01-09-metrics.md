---
layout: post
title: Metrics
subtitle: Types | Differences | Takeaway
image: /img/metrics.jpeg
tags: [data science, machine learning]
comments: true
---

Metric is a measure of the effectiveness of a model. The effectiveness of a model depends on the problem type and is unique. So metric is different for all kinds of problems.

## Regression metrics

- **MSE**: Measures average/mean squared error of our predictions. At each point, calculate the squared difference between predictions and target and average those values over the number of objects.

	<img src="/img/MSE.jpeg" alt="MSE" style="width: 200px;"/>

    What if we do not use the features to predict target values? What if we predict the *same value (optimal value)* for all the objects? The optimal value that reduces the MSE is the *mean* value of the target column. The model that predicts optimal value is the *baseline model*.

    To illustrate, consider the following target values:
    ```python
    In [7]: pd.DataFrame([5, 9, 8, 6, 27], columns = ['y'])
    Out[7]: 
        y
    0   5
    1   9
    2   8
    3   6
    4  27
    ```
    To build a baseline model, we should find that one constant value which we predict irrespective of the parameters or features. It is is (55/5) i.e. 11. This means, if our prediction is 11 for all objects (rows), we will get the least MSE, which will be our baseline model.

- **RMSE**: Root Mean Square Error is very similar to MSE where we take square root of MSE. Scale of errors and the target values are same. It will be easy to comprehend the error as it is linear now.

    RMSE and MSE are similar in a way where every minimizer of MSE is a minimizer of RMSE and vice-versa.
    ```python
    MSE(a) > MSE(b) <=> RMSE(a) > RMSE(b)
    ```

    But they are different when we calculate the gradient of RMSE w.r.t i'th prediction which is equal to the gradient of MSE multiplied by some value. This value is called the *learning rate*. In simple words, it means traveling along MSE gradient is equivalent to traveling along RMSE gradient with different learning rate. 

> What would it mean to say, I have a value of 27 for MSE and 0.8 for RMSE? Should I improve my model? 

*So it is hard to just come to a conclusion with just absolute values. It depends upon properties of the dataset and the variation in the target vector. We should be interested in knowing how better is our model when compared to constant baseline model!*

- **R-squared**: This metric scores the model 1 if our model is perfect, 0 otherwise.  

	<img src="/img/r-squared.jpeg" alt="R-squared" style="width: 400px;"/>

        If MSE is 0 (No error), R-squared is 1.  
        If MSE is 1 (Worst model), R-squared is 0.  

	All models will score between 0 and 1. If we optimize MSE, we are optimizing R-squared.

> The above 3 metrics are similar from an optimization point of view.

- **MAE**: Mean Absolute Error is calculated by averaging the distances(difference) between the absolute target values and the predicted values. So, it doesn't penalize heavily for huge errors as in case of MSE. So, it is not as sensitive as MSE or influenced by any outliers.

	<img src="/img/MAE.jpeg" alt="MAE" style="width: 200px;"/>

    It is used in the financial domain where $10 error is double that of $5 error. In contrast to MSE thinks that $10 is 4 times worse than $5 error.

    The optimal value that reduces the MSE is the *median* value of the target column. This is the baseline model for MAE.

    To illustrate, consider the following target values:
    ```python
    In [7]: pd.DataFrame([5, 9, 8, 6, 27], columns = ['y'])
    Out[7]: 
        y
    0   5
    1   9
    2   8
    3   6
    4  27
    ```
    To build a baseline model with MAE, we should find that one constant value which we predict irrespective of the parameters or features. It is 8.

> If you are sure that data has outliers, choose MAE.  
If you think they are unexpected values, and needs to be considered to build the model, choose MSE.

*MSE and MAE work with absolute errors. Lets have a look at the following tables.*

| TABLE 1       | predicted           | sold  | MSE |
| ------------- |:-------------:| -----:| -----:|
| **shop1**      | 9 | 10 | 1|
| **shop2**      | 999      |   1000 | 1|
| **shop3**      | 900      |   1000 | 10000|

*As, it is noticeble that the error in prediction for shop1 is more critical than shop2, MSE come up with same metric.*

| TABLE 2     | predicted           | sold  | Relative metric |
| ------------- |:-------------:| -----:| -----:|
| **shop1**      | 9 | 10 | 1|
| **shop2**      | 900      |   1000 | 1|

*Table 2 is more intuitive. The relative metric errors can be expressed as following metric calculations*

- **MSPE**: Mean Square Percentage Error, the difference is quite evident where the difference is divided by target-square value which gives us the relative error. This division by it's target-square can also be read as adding weight to MSE.

	<img src="/img/MSPE.jpeg" alt="MSPE" style="width: 300px;"/>

	The optimal value that reduces the MSPE is the *weighted mean* value of the target column. This is the baseline model for MSPE.

	>Also called weighted version of MSE.

- **MAPE**: Mean Absolute Percentage Error, as of MSPE, the difference is divided by target value which gives us the relative error.

	<img src="/img/MAPE.jpeg" alt="MAPE" style="width: 300px;"/>

	The optimal value that reduces the MAPE is the *weighted median* value of the target column. This is the baseline model for MAPE.

	>Also called weighted version of MAE.

- **(R)MSLE**: (Root) Mean Square Logarithmic Error, is RMSE calculated in lagarithmic scale.

	<img src="/img/RMSLE.jpeg" alt="RMSLE" style="width: 350px;"/>

	We take log values of target and predicted values and compute RMSE between them.

	<img src="/img/alsoRMSLE1.jpeg" alt="alsoRMSLE1" style="width: 250px;"/>
	<img src="/img/alsoRMSLE2.jpeg" alt="alsoRMSLE2" style="width: 250px;"/>

	The optimal value that reduces the RMSLE is the *weighted mean* value of the target column which is inverse transformed to the normal one. This is the baseline model for RMSLE.

	>It is just MSE in log space.

## Classification metrics

- **