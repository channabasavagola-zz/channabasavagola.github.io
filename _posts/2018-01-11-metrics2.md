---
layout: post
title: Metrics (Cont...)
subtitle: Types | Differences | Takeaway
image: /img/metrics.jpeg
tags: [data science, machine learning]
comments: true
---

I suggest you to go through [metrics](https://channabasavagola.github.io/2018-01-09-metrics/) as much of the content will be continuation from there.

## Classification metrics

*Soft predictions/labels*: These are the probability values that a classifier output. This vector will be of size equal to number of classes.

*Hard predictions/labels*: But usually classifier will be asked to come up with a label that a particular object belongs to.
This is hard prediction. This can be done in two ways and the choice depends upon the task/problem.
- arg max function f(x): Where you predict the label with the max soft prediction.
- threshold: For binary classification it can be [f(x) > b], where you predict a certain label for the soft prediction values above/below threshold.

**Accuracy**: This works on hard predictions. You will get to know the "percentage of correct predicted labels".
If the dataset has 90% 'label1' and you just predict 'label1' for all the objects. Your accuracy is pretty high. This is misleading as your model has learned nothing.
This also doesn't consider the confidence of the model when it was considering soft predictions to make a hard prediction.

> We need to consider metrics that consider soft predictions and are easier to optimize.

**Logarithmic loss**: This works on soft prediction. Log loss score is kind of penalty for the classification. For pretty bad prediction logloss penalizes hugely.
Minimizing log loss maximizes accuracy (expect a higher score).

>Before AUC ROC, we need to learn few terminologies and there layman definitions which are pretty important in data science.
Following is a self explanatory **confusion matrix**. You should be able to come up with your own definition from the matrix
 for the terminologies **True Negative**, **False Negative**, **True Positive**, and **False Positive**.

| Actual/Prediction| Predicted : No          | Predicted : Yes  |
| ------------- |:-------------:| -----:| -----:|
| **Actual: No**     | True Negative | False Positive |
| **Actual: Yes**    | False Negative    |   True Positive |


> I have attached a self explanatory image from Wikipedia for the terminologies **Precision** and **Recall**.
I find this to be the best one to avoid confusion and also to remember.

<img src="/img/precisionRecall.png" alt="precisionRecall" style="width: 250px;"/>

>So, **Recall** is also called as **Sensitivity**, **Probability of detection** and **True Positive Rate** which is very important to understand AUCROC.
One more terminology **False Positive Rate** is important.
 - **False Positive Rate** = *False Positive/Actual: No*
 **[Specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity), [prevalence](http://conflict.lshtm.ac.uk/page_129.htm), [F score](https://en.wikipedia.org/wiki/F1_score)** are worth mentioning.

**AUC ROC**: This is for unbalanced dataset (Most objects of particular label). *Receiver Operating Characteristic Curve(ROC)* is obtained when you plot *true positive rate(y-axis)* against *false positive rate(x-axis)* for different classification threshold.
Area Under Curve (AUC) is the area under ROC. You can select the threshold with highest AUC.
For worst classification, AUC is 0.5. Best(ideal) classification AUC is 1. 0.8 - 0.9 AUC is considered good.

<img src="/img/ROC.jpeg" alt="ROC" style="width: 200px;"/>

**References**
- For [ROC](http://gim.unmc.edu/dxtests/roc3.htm).