---
layout: post
title: Metrics (Cont...)
subtitle: Types | Differences | Takeaway
image: /img/metrics.jpeg
tags: [data science, machine learning]
comments: true
---

I suggest you to go through [metrics](https://channabasavagola.github.io/2018-01-09-metrics/) as much of the content will be continuation from there.

## Classification metrics

*Soft predictions/labels*: These are the probability values that a classifier output. This vector will be of size equal to number of classes.

*Hard predictions/labels*: But usually classifier will be asked to come up with a label that a particular object belongs to.
This is hard prediction. This can be done in two ways and the choice depends upon the task/problem.
- arg max function f(x): Where you predict the label with the max soft prediction.
- threshold: For binary classification it can be [f(x) > b], where you predict a certain label for the soft prediction values above/below threshold.

**Accuracy**: This works on hard predictions. You will get to know the "percentage of correct predicted labels".
If the dataset has 90% 'label1' and you just predict 'label1' for all the objects. Your accuracy is pretty high. This is misleading as your model has learned nothing.
This also doesn't consider the confidence of the model when it was considering soft predictions to make a hard prediction.

> We need to consider metrics that consider soft predictions and are easier to optimize.

**Logarithmic loss**: This works on soft prediction. Log loss score is kind of penalty for the classification. For pretty bad prediction logloss penalizes hugely.
Minimizing log loss maximizes accuracy (expect a higher score).

>Before AUC ROC, we need to learn few terminologies and there layman definitions which are pretty import in data science.  
Following is a self explainatory **confusion matrix**. You should be able to come up with your own definition from the matrix
 for the terminologies **True Negative**, **False Negative**, **True Positive**, and **False Positive**.

| Actual/Prediction| Predicted : No          | Predicted : Yes  |
| ------------- |:-------------:| -----:| -----:|
| **Actual: No**     | True Negative | False Positive |
| **Actual: Yes**    | False Negative    |   True Positive |

> I have attached a self explainatory image from Wikipedia for the terminologies **Precision** and **Recall**.
I found this to be the best one to avoid confusion and also to remember.

<img src="/img/precisionRecall.png" alt="precisionRecall"/> So, this will be here. I want to see where will this be printed.

**AUC ROC**: Receiver Operating Characteristic Curve(ROC) obtained when you plot true positive rate(y-axis) on false positive rate(x-axis) for different classification threshold.
